{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import jsonlines\n",
    "import json\n",
    "import chardet\n",
    "\n",
    "import dateutil.parser\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used to calculate the run time of a run through\n",
    "# It is used, as I am working with rather large files and it helps me stay on top of planning the runs\n",
    "# as well as my computer reaching it's limit or needing some time to cool off\n",
    "start_time=dateutil.parser.parse(datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only used when I'm doing closer inspection on the text bodies of the tweets\n",
    "# pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning of split files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally I had daily files of tweets regarding the war in Ukraine. As those files were too large to process, I split them via Windows PowerShell into smaller files of a maximum of 800'000 tweets each.  \n",
    "\n",
    "Those split files were then used for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a text file as a log for how many Tweets end up in the cleaned files as well as the number of Tweets per keyword\n",
    "Cleaning_Log_File = f'Cleaning_Log_File.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entering file name without the data type ending, because I will use this part to create the new file names later on\n",
    "file_name = 'input_file_name'\n",
    "# Data type ending is added here\n",
    "input_file = file_name +'.json'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep a better overview, as I worked with a lot of files, I created a log-file. I decided to use a conventional text file for this, but one could also create another type of file, for example a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending log file with name of tweet file\n",
    "with open(Cleaning_Log_File, \"a\") as file:\n",
    "    file.write(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read in data in jsonl format\n",
    "# Although it sais data is a json file, it turend out to be jsonl format.\n",
    "# Therefore, json.load() could not be used\n",
    "def load_jsonl(path):\n",
    "    data=[]\n",
    "    # encoding utf-8-sig is used, as there are some special characters in the data that lead to problems when using utf-8\n",
    "    with open(path, 'r', encoding='utf-8-sig', errors='ignore') as reader: \n",
    "        for line in reader:\n",
    "            data.append(json.loads(line))\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data=load_jsonl(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending log file with total number of Tweets\n",
    "with open(Cleaning_Log_File, \"a\") as file:\n",
    "    file.write('\\nTotal Length: '+str(len(json_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for English tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter for English tweets\n",
    "def only_english(data):\n",
    "    json_data_en = []\n",
    "    for entry in data:\n",
    "        if \"en\" in entry[\"lang\"]:\n",
    "            json_data_en.append(entry)\n",
    "    \n",
    "    return json_data_en\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_en = only_english(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(json_data_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending log file with number of English Tweets\n",
    "with open(Cleaning_Log_File, \"a\") as file:\n",
    "    file.write('\\nEnglish Tweets: '+str(len(json_data_en)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_en[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for the data I want to keep per tweet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of data per tweet that is not needed. To reduce the data, anything that are not related to the tweets text, time of creation, identification, user, language or reetweet and favorite status, are removed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data):\n",
    "    filtered_data = []\n",
    "    for tweet in data:\n",
    "        if \"retweeted_status\" in tweet.keys():\n",
    "            tweet_data = {\n",
    "                \"id\": tweet[\"id\"],\n",
    "                \"id_str\": tweet[\"id_str\"],\n",
    "                \"created_at\": tweet[\"created_at\"],\n",
    "                \"full_text\": tweet[\"full_text\"],\n",
    "                \"retweeted_status\": tweet[\"retweeted_status\"],\n",
    "                \"user\": tweet[\"user\"],\n",
    "                \"retweet_count\": tweet[\"retweet_count\"],\n",
    "                \"favorite_count\": tweet[\"favorite_count\"],\n",
    "                \"favorited\": tweet[\"favorited\"],\n",
    "                \"retweeted\": tweet[\"retweeted\"],\n",
    "                \"lang\": tweet[\"lang\"]\n",
    "                }\n",
    "        else:\n",
    "            tweet_data = {\n",
    "                \"id\": tweet[\"id\"],\n",
    "                \"id_str\": tweet[\"id_str\"],\n",
    "                \"created_at\": tweet[\"created_at\"],\n",
    "                \"full_text\": tweet[\"full_text\"],\n",
    "                \"user\": tweet[\"user\"],\n",
    "                \"retweet_count\": tweet[\"retweet_count\"],\n",
    "                \"favorite_count\": tweet[\"favorite_count\"],\n",
    "                \"favorited\": tweet[\"favorited\"],\n",
    "                \"retweeted\": tweet[\"retweeted\"],\n",
    "                \"lang\": tweet[\"lang\"]\n",
    "                }\n",
    "            \n",
    "        filtered_data.append(tweet_data)\n",
    "    \n",
    "    return filtered_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filter_data(json_data_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if everything looks as it should\n",
    "filtered_data[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column for the full retweet text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For retweets over 140 characters, the \"full_text\" does not show the complete content. But retweets contain the full text of the original tweet under \"full_text\" nested in the \"retweeted_status\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_data = pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the text is nested in the data, this is a function to retrieve all the full retweet texts as a list\n",
    "def get_retweeted_text(data):\n",
    "    list_retweeted_text = []\n",
    "    i=0\n",
    "    for tweets in data:\n",
    "        if \"retweeted_status\" in tweets.keys():\n",
    "            retweeted_text = data[i][\"retweeted_status\"][\"full_text\"]\n",
    "            list_retweeted_text.append(retweeted_text)\n",
    "            i+= 1\n",
    "            \n",
    "        else:\n",
    "            # as I want to add this list as a new column to my dataframe, I cannot skip lines, that aren't retweets\n",
    "            # to not mess up the order of the texts\n",
    "            list_retweeted_text.append(\"-\")\n",
    "            i+= 1\n",
    "            \n",
    "    \n",
    "    return list_retweeted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving a list of the retweeted texts using my function\n",
    "list_retweeted_text = get_retweeted_text(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_retweeted_text[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the list as a column to my dataframe\n",
    "df_filtered_data[\"retweeted_status_full_text\"] = list_retweeted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if it matches \n",
    "df_filtered_data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_data = df_filtered_data.drop('retweeted_status', axis=1)\n",
    "\n",
    "# or alternatively:\n",
    "# df_filtered_data = df_filtered_data.drop(columns='retweeted_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I want to check and filter out potential duplicates. There should not be any, but you never know and it is definitely better to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates = df_filtered_data.drop_duplicates('id_str',keep='first')\n",
    "# I have to go for id_str, as it is more reliable than id -> id as an integer gets corrupted sometimes because of its lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending log file with number of unique English Tweets\n",
    "with open(Cleaning_Log_File, \"a\") as file:\n",
    "    file.write('\\nEnglish Tweets w/o duplicates: '+str(len(df_no_duplicates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving cleaned data as a json file.  \n",
    "\n",
    "When working with lage data, this will take a bit of time. Therefore, this step was not used for all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = df_no_duplicates.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data into file\n",
    "#with open(file_name+\"_clean.json\", \"w\") as twitter_data_file:\n",
    " #   json.dump(output, twitter_data_file, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Tweets regarding the chosen keywords: war, sanction, invasion and humanitarian crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keeping data relevant to create the count from here on\n",
    "df = df_no_duplicates.filter([\"id_str\",\"created_at\",\"full_text\",\"retweeted_status_full_text\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### War"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the keyword \"war\" I had to add a whitespace after the keyword -> \"war_\".  \n",
    "This was necessary, as otherwise I would have gotten many tweets containing words like \"warm\" or \"warning\" instead of \"war\", because \"war\" is used in a variety of different words. By making sure, nothing comes after \"war\", the results only contain war-related tweets, as there are only very few, seldomly used words ending with war, that are not war related. A better solution would have been to use natural language processing methods, but this was not a feasible option within the time limits of this project for the amount of data that would have had to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe only with the tweets containing \"war\" in full_text and/or the added retweeted_status_full_text\n",
    "df_war = df[df['full_text'].str.contains(\"war \") | df['retweeted_status_full_text'].str.contains(\"war \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_war[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_war)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = df_war.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the slimmed down data of tweets containing the keyword \"war\" into seperate \"war\"-file\n",
    "with open(file_name+\"_war.json\", \"w\") as twitter_data_file:\n",
    "    json.dump(output, twitter_data_file, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending log file with number of tweets containing the string \"war\"\n",
    "with open(Cleaning_Log_File, \"a\") as file:\n",
    "    file.write('\\nWar Tweets: '+str(len(df_war)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe only with the tweets containing \"sanction\"\n",
    "df_sanction = df[df['full_text'].str.contains(\"sanction\") | df['retweeted_status_full_text'].str.contains(\"sanction\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sanction[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sanction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = df_sanction.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the slimmed down data of tweets containing the keyword \"sanction\" into seperate \"sanction\"-file\n",
    "with open(file_name+\"_sanction.json\", \"w\") as twitter_data_file:\n",
    "    json.dump(output, twitter_data_file, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending log file with number of tweets containing the string \"sanction\"\n",
    "with open(Cleaning_Log_File, \"a\") as file:\n",
    "    file.write('\\nSanction Tweets: '+str(len(df_sanction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe only with the tweets containing \"invasion\"\n",
    "df_invasion = df[df['full_text'].str.contains(\"invasion\") | df['retweeted_status_full_text'].str.contains(\"invasion\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invasion[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_invasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = df_invasion.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the slimmed down data of tweets containing the keyword \"invastion\" into seperate \"invasion\"-file\n",
    "with open(file_name+\"_invasion.json\", \"w\") as twitter_data_file:\n",
    "    json.dump(output, twitter_data_file, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending log file with number of tweets containing the string \"invasion\"\n",
    "with open(Cleaning_Log_File, \"a\") as file:\n",
    "    file.write('\\nInvasion Tweets: '+str(len(df_invasion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Humanitarian Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe only with the tweets containing \"humanitarian crisis\"\n",
    "df_humanitarian_crisis = df[df['full_text'].str.contains(\"humanitarian crisis\") | df['retweeted_status_full_text'].str.contains(\"humanitarian crisis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_humanitarian_crisis[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_humanitarian_crisis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = df_humanitarian_crisis.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the slimmed down data of tweets containing the keyword \"humanitarian crisis\" into seperate \"humanitarian crisis\"-file\n",
    "with open(file_name+\"_humanitarian_crisis.json\", \"w\") as twitter_data_file:\n",
    "    json.dump(output, twitter_data_file, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending log file with number of tweets containing the string \"humanitarian crisis\"\n",
    "with open(Cleaning_Log_File, \"a\") as file:\n",
    "    file.write('\\nHumanitarian Crisis Tweets: '+str(len(df_humanitarian_crisis))+'\\n')\n",
    "    file.write('\\n ---------------------------------------- \\n \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part is used to check the time the code needed to run. As a large quantity of large files was processed, this helped planning run times and served as an indicator, that I had to stop other processes I was running simultaneously to not exhaust my machine and crash it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time= dateutil.parser.parse(datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_total = end_time - start_time\n",
    "time_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
