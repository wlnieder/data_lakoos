{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import *\n",
    "import unicodedata\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_token = '<Your_Twitter_Bearer_Token>' # In your own environment, i would recommend to save your token as an environment variable\n",
    "s3 = boto3.client('s3')  \n",
    "bucket ='<Name_Of__Your_S3_Bucket>'\n",
    "\n",
    "def lambda_handler(event, context): # Given for AWS Lambda\n",
    "    \n",
    "    # Creating a function that uses the token for authentication and returns headers to access API\n",
    "    def create_headers(bearer_token):\n",
    "        headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "        return headers\n",
    "    \n",
    "    # Request for endpoint with parameters to pass  \n",
    "    def create_url(keyword, start_date, end_date, max_results = 5):\n",
    "        search_url = 'https://api.twitter.com/2/tweets/search/recent'\n",
    "        # the query params can be adapted to collect the data needed\n",
    "        query_params = {'query': keyword,\n",
    "                        'start_time': start_date,\n",
    "                        'end_time': end_date,\n",
    "                        'max_results': max_results,\n",
    "                        'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                        'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                        'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                        'next_token': {}} # This is very important, as it collects the token for the next request page\n",
    "        return (search_url, query_params)\n",
    "    \n",
    "    # Connecting to the endpoint\n",
    "    def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "        params['next_token'] = next_token   #params object received from create_url function\n",
    "        response = requests.request('GET', url, headers = headers, params = params)\n",
    "        print('Endpoint Response Code: ' + str(response.status_code))\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.status_code, response.text)\n",
    "        return response.json()\n",
    "    \n",
    "    # Input \n",
    "    bearer_token = b_token\n",
    "    headers = create_headers(bearer_token)\n",
    "    keyword = 'lang:en -is:retweet -is:reply -is:quote (\"sanctions\")' # only english tweets, no retweets, \n",
    "    # no replies, no quotes and the Tweets have to contain the keyword 'sanctions'\n",
    "    # can be adapted accordingly\n",
    "    start_time = (datetime.now() - timedelta(days=1, hours=3)).isoformat()[:-3]+\"Z\"\n",
    "    end_time = (datetime.now() - timedelta(hours=3)).isoformat()[:-3]+\"Z\"\n",
    "    max_results = 100  # has to be between 10 and 100 -> limitations for number of Tweets per request by Twitter\n",
    "    data = []\n",
    "\n",
    "    jsonFile = f'twitter_data_{datetime.utcnow().strftime(\"%Y%m%d_%H_%M_%fZ\")}.json' \n",
    "    # UTC time included in file name, to identify the time period of collected Tweets by the file name\n",
    "\n",
    "    count = 0 \n",
    "    max_count = 45000 # Maximum of Tweets to be collected in total\n",
    "    next_token = None  \n",
    "\n",
    "    while count < max_count:\n",
    "         # The following 2 lines are optional: security break in case of a mix up with the Tweet numbers\n",
    "        if count+max_results > max_count:\n",
    "            break   \n",
    "    \n",
    "        print('-------------------')\n",
    "        print('Token: ', next_token)\n",
    "        url = create_url(keyword, start_time,end_time, max_results)\n",
    "        json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "        result_count = json_response['meta']['result_count']\n",
    "        entry = json_response\n",
    "        \n",
    "        if 'next_token' in json_response['meta']:\n",
    "            # Save the token to use for next call\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print('Next Token: ', next_token)\n",
    "            if result_count is not None and result_count > 0 and next_token is not None: \n",
    "                print('Start Date: ', start_time)\n",
    "                data.append(entry)\n",
    "                count += result_count\n",
    "                print('Total # of Tweets added: ', count) # To keep track of number of Tweets collected during the process\n",
    "                print('-------------------')\n",
    "                uploadByteStream = bytes(json.dumps(data).encode('UTF-8'))\n",
    "\n",
    "                        \n",
    "        # If no next token exists\n",
    "        else:\n",
    "            if result_count is not None and result_count > 0: # If there is no next token anymore aka the last request page was reached\n",
    "                print('-------------------')\n",
    "                print('Start Date: ', start_time)\n",
    "                data.append(entry)\n",
    "                count += result_count\n",
    "                print('Total # of Tweets added: ', count)\n",
    "                print('-------------------')\n",
    "                next_token = None\n",
    "                uploadByteStream = bytes(json.dumps(data).encode('UTF-8'))\n",
    "                s3.put_object(Bucket=bucket, Key=jsonFile, Body=uploadByteStream)\n",
    "\n",
    "        time.sleep(0.5)  # To slow down requests \n",
    "        # -> should be adapted according to number of requests and runtime of the code to stay within Twitter request limits\n",
    "\n",
    "        \n",
    "    print('Total number of results: ', count) # To see, how many Tweets were collected in total\n",
    "    print(jsonFile)      # To see, in which datafile, the Tweets are collected\n",
    "    print('Put Complete')\n",
    "    print('-------------------')\n",
    "    \n",
    "    return {\n",
    "        'Total number of results: ': count,\n",
    "        'File name': jsonFile\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
