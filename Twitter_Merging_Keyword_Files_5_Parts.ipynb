{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge files with 5 parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is used after the Twitter_Cleaning_And_Filtering code and before the Twitter_Merging_Total_And_Creating_Count. It is used to merge the files containing only tweets with specific keywords into daily files again. This step inbetween was necessary, as not all files had the same amount of parts and had slight differences in their names. \n",
    "\n",
    "I had on average five files per day, but also 2 days with four files, where I tried the whole process with 1 mio. tweets per file, as well as a couple days with six files. The code was pretty much the same, only the function merge_keywords_5_parts was adapted to 4 or 6 files respectively.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entering file name without the data type ending, because I will use this part to create the new file names later on\n",
    "file_name = 'input_file_name'\n",
    "# Data type ending is added here\n",
    "output_file_name='ukraine_russia-2022-03-23_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Humanitarian Crisis - daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file name has been defined above. \n",
    "# If i wanted to use this function, for several file names, i would define the file name as a passing variable\n",
    "# But in this case, this was the simpler solution\n",
    "\n",
    "def merge_keywords_5_parts(keyword):\n",
    "    i = 1\n",
    "\n",
    "    while i < 6:\n",
    "        if i == 1: \n",
    "            part=i\n",
    "            input_file = file_name+str(part)+keyword+'.json'\n",
    "            with open(input_file,'r') as f:\n",
    "                data = json.loads(f.read())\n",
    "            list_keyword = data\n",
    "            i+=1\n",
    "    \n",
    "        else:\n",
    "            part=i\n",
    "            input_file = file_name+str(part)+keyword+'.json'\n",
    "            with open(input_file,'r') as f:\n",
    "                data = json.loads(f.read())\n",
    "            list_keyword.extend(data)\n",
    "            i+=1\n",
    "        \n",
    "    return list_keyword\n",
    "\n",
    "# The merged parts are returned as a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the created function by passing the keyword ending \n",
    "list_humanitarian_crisis = merge_keywords_5_parts('_humanitarian_crisis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_humanitarian_crisis[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_humanitarian_crisis = pd.DataFrame(list_humanitarian_crisis, columns=['id_str', 'created_at','full_text','retweeted_status_full_text'])\n",
    "df_humanitarian_crisis[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_humanitarian_crisis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping potential duplicates\n",
    "df_humanitarian_crisis.drop_duplicates('id_str',keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the merged parts as a daily file\n",
    "df_humanitarian_crisis.to_csv(output_file_name+'humanitarian_crisis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Invasion - daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_invasion = merge_keywords_5_parts('_invasion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invasion = pd.DataFrame(list_invasion, columns=['id_str', 'created_at','full_text','retweeted_status_full_text'])\n",
    "df_invasion[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_invasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invasion.drop_duplicates('id_str',keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invasion.to_csv(output_file_name+'invasion.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Sanction - daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sanction = merge_keywords_5_parts('_sanction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sanction = pd.DataFrame(list_sanction, columns=['id_str', 'created_at','full_text','retweeted_status_full_text'])\n",
    "df_sanction[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sanction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sanction.drop_duplicates('id_str',keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sanction.to_csv(output_file_name+'sanction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge War - daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_war = merge_keywords_5_parts('_war')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_war = pd.DataFrame(list_war, columns=['id_str', 'created_at','full_text','retweeted_status_full_text'])\n",
    "df_war[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_war)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_war.drop_duplicates('id_str',keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_war.to_csv(output_file_name+'war.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
